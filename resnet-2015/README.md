# paper-impl

## 概要
本リポジトリは、Kaiming Heらによる論文「Deep Residual Learning for Image Recognition」(2015) に基づき、ResNetアーキテクチャをゼロから実装したものです。  
目的は、現代的なCNNにおける重要な概念であるResidual Connectionの理論的背景と、その効果をコードレベルで深く理解することです。本実装はCIFAR-10データセットを用いて学習・評価を行いました。

## 動作環境


## 実行方法
python main.py --epochs 50 --batch_size 128

## 実験結果
CIFAR-10データセットで50エポック学習させた結果、以下の性能を達成しました。
- 学習曲線
- 最終的な精度
## ファイル
- model.py - resnetのアーキテクチャ
- train.py - トレーニングループ
- main.py - エントリーポイント

## 学んだこと
Residual Connectionが勾配消失を解消した理由  

- 普通のCNNの場合  

層lの出力xlが、次の層l+1に入力されるとする。活性化関数をf、重みをWl+1とすると  
$$x_{l+1} = f(W_{l+1}・x_l)$$  
最終的な損失Lをxlで微分した勾配(xlに伝わってきた勾配)は  
$$\frac{\partial L}{\partial x_l} = \frac{\partial L} {\partial x_{l+1}} ・\frac{\partial x_{l+1}}{\partial x_{l}} $$  
最後の項に着目し、これを計算すると
$$\frac{\partial x_{l+1}}{\partial x_l} = f'(W_{l+1}・x_l)・W_{l+1}$$
となる。つまり勾配は一つ前の層に伝播するたびに活性化関数の微分f'と重みWが乗算される。  
これが50層繰り返されるとどうなるか  

$$\frac{\partial L}{\partial x_1} = \frac{\partial L}{\partial x_{50}}・(f'_{49}W50)・(f'_{48}W_{49})・・・・(f'_1W_2)$$

活性化関数を微分したものは1よりも小さいものが多いので、ほぼ0になってしまう  

- ResNetの場合　　

ある層の出力をH(x)としたとき、普通のネットワークはH(x)そのものを学習しようとするが、ResNetは出力と入力の差分(Residual)であるF(x)を学習する(後述する)  

$$H(x) = F(x) + x$$

ある層の出力xlがResNesブロックに入力され、出力xl+1になるとする。 
$$x_{l+1} = x_l + F(x_l,W_{l+1})$$  
この式の勾配を計算すると、  
$$\frac{\partial L}{\partial x_l} = \frac{\partial L}{\partial x_{l+1}}・\frac{\partial x_{l+1}}{\partial x_l}$$  
$$\frac{\partial x_{l+1}}{x_l} = \frac{\partial}{\partial x_l}(x_l + F(x_l,W_{l+1}))$$   
$$= \frac{x_l}{x_l} + \frac{\partial F(x_l,W_{l+1})}{\partial x_l}$$  
$$= 1 + \frac{\partial F(x_l,W_{l+1})}{\partial x_l}$$  
これがResNetの本質である。

乗算されても1以上であるため勾配消失問題が起こらない。

- なぜx足しただけで、ネットワークF(x)は差分を学習するようになるのか  
結論：損失関数の形が変わり、それによって勾配の計算結果が変わるから

1.普通のネットワークの場合  
予測  
$$y_{pred} = F(x,W)$$  
損失  
$$L = \frac{(F(x,W)-y_{true})^2}{2}$$
$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial F}・\frac{\partial F}{\partial W}$$  
$$= (F(x,W) - y_{true})・\frac{\partial F(x,W)}{\partial W}$$  
勾配の大きさを決めている主要な項はF(x,W)-y_trueである。
学習とはこの勾配を0に近づけるプロセス、つまりF(x,W) - y_trueが0になるように重みWを更新していく。  
これは、F(x,W)がy_trueに近づくように学習が進むことを意味する。
つまり、ネットワークFは正解y_trueそのものを直接出力するように学習する。  

2.ResNetの場合  
予測
$$y_{pred} = F(x,W) + x$$  
損失  
$$\frac{((F(x,W)+x)-y_{true})^2}{2}$$  
$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y_{pred}}・\frac{\partial y_{pred}}{\partial 
W}$$  
$$=(y_{pred} - y_{true})・\frac{\partial (F(x,W)+x)}{\partial W}$$  
$$=((F(x,W)+x)-y_{true})・\frac{\partial F(x,W)}{\partial W}$$  
勾配の大きさを決めている主要な項が((F(x,W) + x) - y_{true})に変わった。  
学習はこの項が0になるように進む、つまり  
$$(F(x,W)+x)-y_{true}→0$$  
$$F(x,W)→y_{true} - x$$  
これはF(x,W)がy_trueと入力xの差分に近づくように学習が進むことを意味する。

以下はgeminiに聞いたResNetの例えである  

写真編集のプロと見習いアシスタント  
ここに、「少し暗い写真」があるとします。あなたの仕事は、この写真を「完璧な明るさの写真」に仕上げることです。あなたはプロの写真編集者です。

- 入力 x: 少し暗い写真（加工前の素材）

- 理想の出力 H(x): 完璧な明るさの写真（正解）

あなたは、この仕事を「ニューラルネットワーク」という名前の見習いアシスタントに任せることにしました。

普通のネットワークのやり方：「ゼロから模写しろ！」
あなたはアシスタント（普通のネットワーク）に、①少し暗い写真 x と ②完璧な写真 H(x) の両方を見せて、こう指示します。

「この暗い写真を参考にして、この完璧な写真をゼロから完全に再現（模写）してくれ。」

この指示を受けたアシスタントは、ピクセルの一つ一つが最終的にどんな色や明るさになるべきかを、全て記憶し、再現しようとします。これは非常に複雑な仕事です。もし元の暗い写真が「ほぼ完璧」に近かったとしても、アシスタントは「ほぼ完璧な状態」をゼロから再現するという、無駄の多い大変な作業を強いられます。

これが、H(x)そのものを学習しようとする、という状態です。ネットワーク（アシスタント）は、最終的な出力H(x)の全体像を直接作り出そうとします。

ResNetのやり方：「変更点だけ教えてくれ！」
あなたは別のアシスタント（ResNet）に、同じく①少し暗い写真 x と ②完璧な写真 H(x) を見せますが、指示の出し方を根本から変えます。

「この暗い写真が、最終的にこの完璧な写真になるには、どんな加工フィルター（変更点）をかければいいか、そのフィルターだけを教えてくれ。」

この指示を受けたResNetアシスタントは、最終的な写真そのものを作ろうとはしません。彼が学習するのは、たった一つのシンプルな指示です。例えば、

「明るさを『+20』するフィルター」

これが、ResNetが学習する差分F(x)です。

そして、あなたはプロとして、アシスタントが作ったその「+20フィルターF(x)」を受け取り、元の「少し暗い写真x」に適用します。

完璧な写真 H(x) = 少し暗い写真 x + 「明るさ+20」フィルター F(x)

数式で書くと H(x) = x + F(x) となります。

なぜResNetのやり方が圧倒的に優れているのか？
この違いが劇的な効果を生むのは、「もし入力が既に完璧だったら？」というケースを考えたときです。

もし、あなたに渡された入力写真 x が、最初から「完璧な明るさの写真」だったらどうでしょう？

普通のネットワークの場合:
アシスタントは「完璧な写真x」から「完璧な写真H(x)」をゼロから再現しろ、と指示されます。入力と出力が同じでも、それを再現するのは依然として難しい仕事です。

ResNetの場合:
アシスタントへの指示は「完璧な写真xを完璧な写真H(x)にするためのフィルターを教えてくれ」です。答えはあまりに明白です。
「何もしない（変更点ゼロ）フィルター」
つまり、アシスタントは F(x) = 0 を学習すれば良いだけです。ニューラルネットワークにとって、「0を出力する」のは非常に簡単な仕事です。